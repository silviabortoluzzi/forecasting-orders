{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adcca74",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2adc49",
   "metadata": {
    "gather": {
     "logged": 1762532206060
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from prophet import Prophet\n",
    "from prophet.make_holidays import make_holidays_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb288af8",
   "metadata": {
    "gather": {
     "logged": 1762532100050
    }
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION & CONSTANTS\n",
    "receivals = pd.read_csv('../data/kernel/receivals.csv')\n",
    "purchase_orders = pd.read_csv(\"../data/kernel/purchase_orders.csv\")\n",
    "mapping = pd.read_csv(\"../data/prediction_mapping.csv\")\n",
    "\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "pred_days = 150 # number of days to predict\n",
    "start_date=\"2009-01-01\" # start date for continuos training data \n",
    "min_factor = 0.6 #minimum shrink factor (applied to the lowest annual quantity items)\n",
    "max_factor = 0.95 #maximum shrink factor (applied to the highest annual quantity items)\n",
    "changepoint_prior_scale= 0.008 #changepoint prior scale for prophet model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59547a",
   "metadata": {},
   "source": [
    "## Preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2742bb51",
   "metadata": {
    "gather": {
     "logged": 1762532206258
    }
   },
   "outputs": [],
   "source": [
    "def receival_cleaned(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df = df.dropna(subset=[\"rm_id\",\"date_arrival\", \"net_weight\"]).copy()\n",
    "    df['date_arrival'] = pd.to_datetime(df['date_arrival'], utc=True)\n",
    "    cols = [\"rm_id\", \"product_id\", \"date_arrival\", \"net_weight\"]\n",
    "\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def po_cleaned(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df[\"delivery_date\"] = pd.to_datetime(df[\"delivery_date\"], errors=\"coerce\", utc=True)\n",
    "    df[\"delivery_date\"] = pd.to_datetime(df[\"delivery_date\"]).dt.date\n",
    "    df = df.dropna(subset=[\"purchase_order_id\", \"delivery_date\", \"quantity\"])\n",
    "    df = df[df[\"quantity\"] > 0]\n",
    "    cols = [\"delivery_date\", \"product_id\", \"quantity\"]\n",
    "\n",
    "    return df[cols]\n",
    "\n",
    "def quantity_agg(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df_agg = (\n",
    "        df.groupby([\"delivery_date\", \"product_id\"])[\"quantity\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"delivery_date\": \"date\", \"quantity\": \"quantity_ordered\"})\n",
    "    )\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "def daily_weight_agg(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df[\"date\"] = df[\"date_arrival\"].dt.floor(\"D\")\n",
    "    df_agg = (\n",
    "        df.groupby([\"rm_id\", \"date\"], as_index=False)[\"net_weight\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"net_weight\": \"daily_weight\"})\n",
    "    )\n",
    "    prod_map = df.groupby(\"rm_id\")[\"product_id\"].first()\n",
    "    df_agg[\"product_id\"] = df_agg[\"rm_id\"].map(prod_map)\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "def orders_timeseries_merged(timeseries_df: pd.DataFrame, orders_agg_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    final_df = pd.merge(timeseries_df, orders_agg_df, on=[\"date\", \"product_id\"], how=\"left\")\n",
    "    final_df[\"quantity_ordered\"] = final_df[\"quantity_ordered\"].fillna(0)\n",
    "    final_df[\"date\"] = pd.to_datetime(final_df[\"date\"])\n",
    "    cols = [\"rm_id\", \"product_id\", \"date\", \"daily_weight\", \"quantity_ordered\"]\n",
    "\n",
    "    return final_df[cols]\n",
    "\n",
    "def timeseries_continuous(df: pd.DataFrame, start_date, end_date) -> pd.DataFrame:\n",
    "   \n",
    "    prod_map = df.groupby(\"rm_id\")[\"product_id\"].first()\n",
    "    all_rms = df[\"rm_id\"].unique()\n",
    "    all_dates = pd.date_range(start=start_date, end=end_date, freq=\"D\", tz=\"UTC\")\n",
    "    \n",
    "    full_index = pd.MultiIndex.from_product([all_rms, all_dates], names=[\"rm_id\", \"date\"])\n",
    "    df = df.set_index([\"rm_id\", \"date\"]).reindex(full_index).reset_index()\n",
    "  \n",
    "    df[\"product_id\"] = df[\"rm_id\"].map(prod_map)\n",
    "    df[\"daily_weight\"] = df[\"daily_weight\"].fillna(0)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_holiday(date, holiday_df):\n",
    "    return date in set(holiday_df['ds'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ec4ad",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa41dbd",
   "metadata": {
    "gather": {
     "logged": 1762532206707
    }
   },
   "outputs": [],
   "source": [
    "receivals = receival_cleaned(receivals)\n",
    "receivals_daily_agg = daily_weight_agg(receivals)\n",
    "\n",
    "timeseries = timeseries_continuous(receivals_daily_agg, start_date=start_date, end_date=\"2025-01-01\")\n",
    "\n",
    "receivals[\"date_arrival\"] = pd.to_datetime(receivals[\"date_arrival\"])\n",
    "receivals_after = receivals[receivals[\"date_arrival\"] >= \"2024-10-01\"]\n",
    "active_ids = set(receivals_after[\"rm_id\"].unique())\n",
    "rm_counts = receivals['rm_id'].value_counts()\n",
    "ids_enough = rm_counts[rm_counts >= 10].index\n",
    "\n",
    "purchase_orders = po_cleaned(purchase_orders)\n",
    "purchase_orders['delivery_date'] = pd.to_datetime(purchase_orders['delivery_date'], utc=True)\n",
    "purchase_orders['year'] = purchase_orders['delivery_date'].dt.year\n",
    "\n",
    "quantity_year = (\n",
    "    purchase_orders\n",
    "    .groupby(['product_id', 'year'], as_index=False)['quantity']\n",
    "    .sum()\n",
    "    .rename(columns={'quantity': 'quantity_year'})\n",
    ")\n",
    "\n",
    "df = timeseries.copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "df = df.merge(quantity_year, on=['product_id', 'year'], how='left')\n",
    "\n",
    "df[\"cum_weight\"] = df.groupby(['rm_id'])['daily_weight'].cumsum()\n",
    "\n",
    "df = df [[\"rm_id\", \"date\", \"quantity_year\", \"cum_weight\"]]\n",
    "df['quantity_year']=df['quantity_year'].fillna(0)\n",
    "\n",
    "df = df[df['rm_id'].isin(active_ids)]\n",
    "df = df[df['rm_id'].isin(ids_enough)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663345fe",
   "metadata": {},
   "source": [
    "## Prophet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba74e7f8",
   "metadata": {
    "gather": {
     "logged": 1762532207264
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:56:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:56:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:05 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for rm_id, group in df.groupby('rm_id'):\n",
    "\n",
    "    tmp = group.rename(columns={\"date\": \"ds\", \"cum_weight\": \"y\"})\n",
    "    active_first = tmp.loc[tmp['y'] > 0, 'ds'].iloc[0]\n",
    "    norway_holidays = make_holidays_df(year_list=tmp['ds'].dt.year.unique(), country='NO')\n",
    "    tmp = tmp[tmp['ds'] >= active_first].reset_index(drop=True)\n",
    "    tmp['working_days'] = (tmp['ds'].dt.weekday < 5).astype(int)\n",
    "\n",
    "    \n",
    "    m = Prophet(\n",
    "        weekly_seasonality = True,\n",
    "        holidays = norway_holidays,\n",
    "        \n",
    "        changepoint_prior_scale = changepoint_prior_scale,  \n",
    "        seasonality_prior_scale = 10.0,      \n",
    "        seasonality_mode = 'additive' \n",
    "        )\n",
    "    \n",
    "    m.add_regressor('working_days')\n",
    "   \n",
    "    m.fit(tmp)\n",
    "\n",
    "    future_df = m.make_future_dataframe(periods=150, freq=\"D\")\n",
    "    future_df['working_days'] = future_df['ds'].dt.weekday < 5\n",
    "    future_df['working_days'] = future_df['working_days'].astype(int)\n",
    "    future_df['quantity_year'] = tmp.iloc[-1]['quantity_year']\n",
    "        \n",
    "    forecast = m.predict(future_df)\n",
    "\n",
    "    forecast['rm_id'] = rm_id\n",
    "    forecast['quantity_year'] = tmp.iloc[-1]['quantity_year']\n",
    "    results.append(forecast[['ds', 'rm_id', 'quantity_year', 'yhat','yhat_lower']])\n",
    "final = pd.concat(results)\n",
    "\n",
    "final = final.merge(df[['rm_id', 'date', 'cum_weight']], left_on=['rm_id', 'ds'], right_on=['rm_id', 'date'], how='left')\n",
    "final = final.drop(columns=['date'])\n",
    "final = final.rename(columns={'cum_weight': 'y'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0510f0",
   "metadata": {},
   "source": [
    "## Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d5c33f",
   "metadata": {
    "gather": {
     "logged": 1762532207368
    }
   },
   "outputs": [],
   "source": [
    "future_df_2025 = final[(final['ds'] >= '2025-01-02') & (final['ds'] <= '2025-12-31')].copy()\n",
    "\n",
    "preds = future_df_2025[\"yhat\"].values.copy()\n",
    "\n",
    "for start in range(0, len(preds), pred_days):\n",
    "    end = start + pred_days\n",
    "    preds[start:end] = np.maximum.accumulate(preds[start:end])\n",
    "    \n",
    "future_df_2025[\"yhat_corr\"] = preds\n",
    "\n",
    "preds = future_df_2025['yhat_corr'].values.copy()\n",
    "predicted_weight = np.zeros_like(preds)\n",
    "\n",
    "for start in range(0, len(preds), pred_days):\n",
    "    end = start + pred_days\n",
    "    base = preds[start]                   \n",
    "    predicted_weight[start:end] = preds[start:end] - base\n",
    "\n",
    "future_df_2025['predicted_weight'] = predicted_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daa65db5",
   "metadata": {
    "gather": {
     "logged": 1762532207445
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "future_df_2025 = future_df_2025[(future_df_2025['ds'] >= '2025-01-02')]\n",
    "\n",
    "q_min = future_df_2025['quantity_year'].min()\n",
    "q_max = future_df_2025['quantity_year'].max()\n",
    "future_df_2025['quantity_year_norm'] = (future_df_2025['quantity_year'] - q_min) / (q_max - q_min)\n",
    "\n",
    "future_df_2025['shrinking'] = min_factor + (max_factor - min_factor) * future_df_2025['quantity_year_norm']\n",
    "\n",
    "\n",
    "future_df_2025['predicted_weight'] = (\n",
    "    future_df_2025['predicted_weight'] * future_df_2025['shrinking']\n",
    ")\n",
    "\n",
    "future_df_2025 = future_df_2025[['ds','predicted_weight','rm_id']]\n",
    "future_df_2025 = future_df_2025.rename(columns={'ds': 'date'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f09937",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcafca4",
   "metadata": {
    "gather": {
     "logged": 1762532207464
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Submission file saved in model.\n"
     ]
    }
   ],
   "source": [
    "mapping[\"forecast_start_date\"] = pd.to_datetime(mapping[\"forecast_start_date\"], utc=True)\n",
    "mapping[\"forecast_end_date\"] = pd.to_datetime(mapping[\"forecast_end_date\"], utc=True)\n",
    "mapping = mapping.rename(columns={'forecast_end_date': 'date'})\n",
    "future_df_2025[\"date\"] = pd.to_datetime(future_df_2025[\"date\"], utc=True)\n",
    "\n",
    "mapping = mapping.merge(\n",
    "    future_df_2025[['rm_id', 'date', 'predicted_weight']],\n",
    "    on=['rm_id', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "submission = mapping[[\"ID\", \"predicted_weight\"]].copy()\n",
    "\n",
    "submission[\"predicted_weight\"] = submission[\"predicted_weight\"].fillna(0)\n",
    "\n",
    "submission.to_csv(\"../submissions/final_submission_prophet.csv\", index=False)\n",
    "print(\"Finished. Submission file saved in model.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "env_all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
