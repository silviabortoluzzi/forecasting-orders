{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import date, timedelta, datetime\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION & CONSTANTS\n",
    "\n",
    "# seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "LBS_TO_KG = 0.453592 # conversion factor from pounds to kilograms\n",
    "HYPEROPT_N_TRIALS = 20 # number of hyperparameter optimization trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    df_rec = pd.read_csv('../data/kernel/receivals.csv')\n",
    "    df_porder = pd.read_csv('../data/kernel/purchase_orders.csv')\n",
    "    df_materials = pd.read_csv('../data/extended/materials.csv')\n",
    "    prediction_mapping = pd.read_csv('../data/prediction_mapping.csv')\n",
    "\n",
    "    # Data Cleaning and Normalization\n",
    "    df_rec = df_rec.dropna(subset=['date_arrival', 'rm_id', 'net_weight'])\n",
    "    df_porder = df_porder.dropna(subset=['delivery_date', 'product_id', 'quantity'])\n",
    "\n",
    "    df_porder['delivery_date'] = pd.to_datetime(df_porder['delivery_date'], utc=True, errors='coerce').dt.date\n",
    "    df_rec['date_arrival'] = pd.to_datetime(df_rec['date_arrival'], utc=True, errors='coerce').dt.date\n",
    "    prediction_mapping['forecast_start_date'] = pd.to_datetime(prediction_mapping['forecast_start_date'], errors='coerce').dt.date\n",
    "    prediction_mapping['forecast_end_date']   = pd.to_datetime(prediction_mapping['forecast_end_date'], errors='coerce').dt.date\n",
    "    \n",
    "    mask_p = (df_porder['unit_id'] == 43) \n",
    "    df_porder.loc[mask_p, 'quantity'] = LBS_TO_KG * df_porder.loc[mask_p, 'quantity']\n",
    "    df_porder.loc[mask_p, 'unit_id'] = 40 \n",
    "    df_porder.loc[mask_p, 'unit'] = 'kg'\n",
    "    \n",
    "    df_rec = df_rec.loc[df_rec['net_weight'] > 0].copy()\n",
    "    df_porder = df_porder.loc[df_porder['quantity'] > 0].copy()\n",
    "\n",
    "    return df_rec, df_porder, df_materials, prediction_mapping\n",
    "\n",
    "\n",
    "def aggregates_3df_daily(df_rec, df_porder, df_materials):\n",
    "    # print(\"Aggregating receivals and purchase orders dataframes on a daily level on rm_id\")\n",
    "\n",
    "    receivals_daily_agg = df_rec.groupby(['rm_id', 'date_arrival']).net_weight.sum().reset_index()\n",
    "    map_product_rm = df_materials[['rm_id', 'product_id']].drop_duplicates().dropna()\n",
    "\n",
    "    df_porder_w_rmid = pd.merge(df_porder, map_product_rm, on='product_id', how='left').dropna(subset=['rm_id'])\n",
    "    daily_po_agg = df_porder_w_rmid.groupby(['rm_id', 'delivery_date']).quantity.sum().reset_index()\n",
    "    daily_po_agg['rm_id'] = daily_po_agg['rm_id'].astype(int)\n",
    "    receivals_daily_agg['rm_id'] = receivals_daily_agg['rm_id'].astype(int)\n",
    "    return receivals_daily_agg, daily_po_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Using Croston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _croston_forecast(y, h=151, alpha=0.1):\n",
    "\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    demand = y[y > 0.0]\n",
    "    if demand.size == 0:\n",
    "        return np.zeros(h, dtype=float)\n",
    "    \n",
    "    intervals = np.diff(np.concatenate(([0], np.where(y > 0)[0])))\n",
    "    z = demand[0]\n",
    "    p = intervals[0] if intervals.size > 0 else 1.0\n",
    "    \n",
    "    for k in range(1, demand.size):\n",
    "        z = alpha * demand[k]   + (1 - alpha) * z\n",
    "    for k in range(1, intervals.size):\n",
    "        p = alpha * intervals[k] + (1 - alpha) * p\n",
    "        \n",
    "    rate = z / max(p, 1e-6) \n",
    "    return np.full(h, rate, dtype=float)\n",
    "\n",
    "\n",
    "def build_intermittent_baseline(daily_receivals, start_date, end_date, method=\"croston\", alpha=0.1):\n",
    "\n",
    "    horizon_days = (end_date - start_date).days + 1\n",
    "    daily_receivals_copy = daily_receivals.copy()\n",
    "    daily_receivals_copy['date_arrival'] = pd.to_datetime(daily_receivals_copy['date_arrival'])\n",
    "    \n",
    "    # Build full daily panel per rm_id\n",
    "    result = []\n",
    "    for rm, group in daily_receivals_copy.groupby('rm_id'):\n",
    "        group = group.set_index('date_arrival').sort_index()\n",
    "        daily_series = group['net_weight'].asfreq('D', fill_value=0.0)\n",
    "        \n",
    "        # Use last N years only to avoid ancient regimes\n",
    "        cutoff = pd.Timestamp(start_date) - pd.Timedelta(days=365*3)\n",
    "        history_series = daily_series[daily_series.index < pd.Timestamp(start_date)]\n",
    "        \n",
    "        if history_series.index.min() is None:\n",
    "            history_series = daily_series[daily_series.index < pd.Timestamp(start_date)]\n",
    "            \n",
    "        history_series = history_series[history_series.index >= cutoff]\n",
    "        \n",
    "        if history_series.empty:\n",
    "            forecast = np.zeros(horizon_days, dtype=float)\n",
    "        else:\n",
    "            if method == \"croston\":\n",
    "                forecast = _croston_forecast(history_series.values, h=horizon_days, alpha=alpha)\n",
    "            else:\n",
    "                forecast = _croston_forecast(history_series.values, h=horizon_days, alpha=alpha) \n",
    "                \n",
    "        dates = pd.date_range(start_date, end_date, freq='D')\n",
    "        result.append(pd.DataFrame({'rm_id': rm, 'date': dates, 'baseline_daily_fc': forecast}))\n",
    "        \n",
    "    return pd.concat(result, ignore_index=True)\n",
    "\n",
    "\n",
    "def sum_baseline_over_windows(features_df, baseline_df):\n",
    "\n",
    "    original_index = features_df.index\n",
    "    rm_ids = features_df['rm_id'].values\n",
    "    start_dates = pd.to_datetime(features_df['forecast_start_date']).values.astype('datetime64[D]')\n",
    "    end_dates   = pd.to_datetime(features_df['forecast_end_date']).values.astype('datetime64[D]')\n",
    "    \n",
    "    # here we build map rm_id -> arrays for vectorized summation\n",
    "    baseline_lookup_map = {}\n",
    "    for k, g in baseline_df.groupby('rm_id'):\n",
    "        dates = pd.to_datetime(g['date']).values.astype('datetime64[D]')\n",
    "        values = g['baseline_daily_fc'].values.astype(float)\n",
    "        baseline_lookup_map[k] = (dates, np.cumsum(values))\n",
    "        \n",
    "    output_sums = np.zeros(len(features_df), dtype=float)\n",
    "    \n",
    "    for i in range(len(features_df)):\n",
    "        lookup_tuple = baseline_lookup_map.get(rm_ids[i])\n",
    "        if lookup_tuple is None: \n",
    "            continue\n",
    "            \n",
    "        dates, cumulative_values = lookup_tuple\n",
    "        left_idx = np.searchsorted(dates, start_dates[i], side='left')\n",
    "        right_idx = np.searchsorted(dates, end_dates[i],   side='right') - 1\n",
    "        \n",
    "        if right_idx >= left_idx and left_idx < len(cumulative_values):\n",
    "            left_sum = cumulative_values[left_idx-1] if left_idx > 0 else 0.0\n",
    "            output_sums[i] = cumulative_values[right_idx] - left_sum\n",
    "            \n",
    "    return pd.Series(output_sums, index=original_index, name='baseline_sum_in_window')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(input_df, daily_receivals, daily_po, receivals_full=None, purchase_orders_full=None):\n",
    "\n",
    "    index_original = input_df.index.name\n",
    "    features_df = input_df.reset_index()\n",
    "    if 'index' not in features_df.columns and index_original is not None:\n",
    "        features_df = features_df.rename(columns={index_original: 'index'})\n",
    "\n",
    "    features_df['window_length'] = (\n",
    "        features_df['forecast_end_date'] - features_df['forecast_start_date']\n",
    "    ).apply(lambda x: x.days + 1).astype(np.int16)\n",
    "\n",
    "    features_df['end_month'] = features_df['forecast_end_date'].apply(lambda x: x.month).astype(np.int16)\n",
    "    features_df['end_dayofweek'] = features_df['forecast_end_date'].apply(lambda x: x.weekday()).astype(np.int16)\n",
    "\n",
    "    features_df['month_sin'] = np.sin(2 * np.pi * features_df['end_month'] / 12).astype(np.float32)\n",
    "    features_df['month_cos'] = np.cos(2 * np.pi * features_df['end_month'] / 12).astype(np.float32)\n",
    "\n",
    "    start_dates_np = pd.to_datetime(features_df[\"forecast_start_date\"]).values.astype('datetime64[D]')\n",
    "    end_dates_np   = pd.to_datetime(features_df[\"forecast_end_date\"]).values.astype('datetime64[D]')\n",
    "    features_df[\"business_days_in_window\"] = [np.busday_count(s, e + np.timedelta64(1, 'D')) for s, e in zip(start_dates_np, end_dates_np)]\n",
    "    features_df[\"business_days_in_window\"] = features_df[\"business_days_in_window\"].astype(np.int16)\n",
    "    features_df[\"weekends_in_window\"] = (features_df[\"window_length\"] - features_df[\"business_days_in_window\"]).astype(np.int16)\n",
    "\n",
    "    end_timestamps = pd.to_datetime(features_df[\"forecast_end_date\"])\n",
    "    features_df[\"end_days_to_month_end\"] = (end_timestamps.dt.days_in_month - end_timestamps.dt.day).astype(np.int16)\n",
    "\n",
    "    #   PO (Purchase Order) in window  \n",
    "    pp = daily_po.copy()\n",
    "    # datetime check\n",
    "    if not np.issubdtype(pp['delivery_date'].dtype, np.datetime64):\n",
    "        pp['delivery_date'] = pd.to_datetime(pp['delivery_date'], errors='coerce')\n",
    "\n",
    "    po_by_rm = {rm: grp.sort_values('delivery_date').reset_index(drop=True)\n",
    "                for rm, grp in pp.groupby('rm_id')}\n",
    "\n",
    "    po_in_window_vals = np.zeros(len(features_df), dtype=np.float32)\n",
    "\n",
    "    for rm, rows in features_df.groupby('rm_id'):\n",
    "        po_grp = po_by_rm.get(rm)\n",
    "        if po_grp is None or po_grp.empty:\n",
    "            continue\n",
    "\n",
    "        row_positions = rows.index.values\n",
    "        po_dates = po_grp['delivery_date'].values.astype('datetime64[D]')\n",
    "        po_qty = po_grp['quantity'].values.astype(float)\n",
    "        po_cum = np.cumsum(po_qty)\n",
    "\n",
    "        starts = pd.to_datetime(rows['forecast_start_date']).values.astype('datetime64[D]')\n",
    "        ends   = pd.to_datetime(rows['forecast_end_date']).values.astype('datetime64[D]')\n",
    "\n",
    "        left_pos  = np.searchsorted(po_dates, starts, side='left')\n",
    "        right_pos = np.searchsorted(po_dates, ends,   side='right') - 1\n",
    "\n",
    "        # sums for cumulative\n",
    "        sums = np.zeros(len(row_positions), dtype=float)\n",
    "        for i, (l, r) in enumerate(zip(left_pos, right_pos)):\n",
    "            if r >= l and r >= 0 and l < len(po_cum):\n",
    "                left_cum = po_cum[l-1] if l > 0 else 0.0\n",
    "                sums[i] = po_cum[r] - left_cum\n",
    "            else:\n",
    "                sums[i] = 0.0\n",
    "        po_in_window_vals[row_positions] = sums\n",
    "\n",
    "    features_df['po_in_window'] = po_in_window_vals\n",
    "    features_df['daily_avg_po_in_window'] = (\n",
    "        features_df['po_in_window'] / features_df['window_length']\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0).astype(np.float32)\n",
    "\n",
    "    # Historical aggregates  \n",
    "    historical_aggregates_list = []\n",
    "    rec_horizons = [7, 30, 90]\n",
    "    po_horizons  = [7, 30, 90]\n",
    "    rec_rolling_windows = [30, 90]\n",
    "\n",
    "    dr = daily_receivals.copy()\n",
    "    dr['date_arrival'] = pd.to_datetime(dr['date_arrival'], errors='coerce')\n",
    "\n",
    "    for start_date in features_df['forecast_start_date'].unique():\n",
    "        reference_date = pd.to_datetime(start_date) - pd.Timedelta(days=1)\n",
    "        rec_frames, po_frames = [], []\n",
    "\n",
    "        for h in rec_horizons:\n",
    "            hist_start = reference_date - pd.Timedelta(days=(h - 1))\n",
    "            hist_rec = dr[(dr['date_arrival'] >= hist_start) & (dr['date_arrival'] <= reference_date)]\n",
    "            rec_agg = hist_rec.groupby('rm_id')['net_weight'].sum()\n",
    "            rec_agg.name = f'hist_rec_{h}d'\n",
    "            rec_frames.append(rec_agg)\n",
    "\n",
    "        for h in po_horizons:\n",
    "            hist_start = reference_date - pd.Timedelta(days=(h - 1))\n",
    "            hist_po = daily_po[\n",
    "                (pd.to_datetime(daily_po['delivery_date']) >= hist_start) &\n",
    "                (pd.to_datetime(daily_po['delivery_date']) <= reference_date)\n",
    "            ]\n",
    "            po_agg = hist_po.groupby('rm_id')['quantity'].sum()\n",
    "            po_agg.name = f'hist_po_{h}d'\n",
    "            po_frames.append(po_agg)\n",
    "\n",
    "        for window in rec_rolling_windows:\n",
    "            hist_start = reference_date - pd.Timedelta(days=(window - 1))\n",
    "            historical_receivals = dr[(dr['date_arrival'] >= hist_start) & (dr['date_arrival'] <= reference_date)]\n",
    "            \n",
    "            rec_mean = historical_receivals.groupby('rm_id')['net_weight'].mean()\n",
    "            rec_mean.name = f'rec_mean_{window}d'\n",
    "            \n",
    "            rec_std = historical_receivals.groupby('rm_id')['net_weight'].std()\n",
    "            rec_std.name = f'rec_std_{window}d'\n",
    "            \n",
    "            def _trend(g):\n",
    "                if len(g) < 2: return 0.0\n",
    "                t = (g['date_arrival'] - g['date_arrival'].min()).dt.days.values\n",
    "                if np.unique(t).size < 2: return 0.0\n",
    "                return float(np.polyfit(t, g['net_weight'].values, 1)[0])\n",
    "            \n",
    "            # future warnings handle\n",
    "            if historical_receivals.empty:\n",
    "                 rec_trend = pd.Series(dtype=float, name=f'rec_trend_{window}d')\n",
    "                 rec_trend.index.name = 'rm_id'\n",
    "            else:\n",
    "                 try:\n",
    "                     trend_result = historical_receivals.groupby('rm_id').apply(_trend, include_groups=False)\n",
    "                 except TypeError:\n",
    "                     trend_result = historical_receivals.groupby('rm_id').apply(_trend)\n",
    "                 if isinstance(trend_result, pd.DataFrame):\n",
    "                     rec_trend = trend_result.iloc[:, 0]\n",
    "                 else:\n",
    "                     rec_trend = trend_result\n",
    "                 rec_trend.name = f'rec_trend_{window}d'\n",
    "            \n",
    "            rec_cnt = historical_receivals.groupby('rm_id')['date_arrival'].nunique()\n",
    "            rec_cnt.name = f'rec_count_{window}d'\n",
    "            \n",
    "            rec_frames.extend([rec_mean, rec_std, rec_trend, rec_cnt])\n",
    "        \n",
    "        all_aggregates = pd.concat(rec_frames + po_frames, axis=1)\n",
    "        all_aggregates = all_aggregates.fillna(0).reset_index()\n",
    "        \n",
    "        all_aggregates['forecast_start_date'] = start_date\n",
    "        historical_aggregates_list.append(all_aggregates)\n",
    "\n",
    "\n",
    "    all_hist_agg = pd.concat(historical_aggregates_list, ignore_index=True)\n",
    "    features_df = features_df.merge(all_hist_agg, on=['rm_id', 'forecast_start_date'], how='left').fillna(0)\n",
    "\n",
    "    features_df['po_to_rec_ratio_30d'] = np.where(features_df['hist_rec_30d'] > 0,\n",
    "                                               features_df['hist_po_30d'] / features_df['hist_rec_30d'], 0).astype(np.float32)\n",
    "    features_df['rec_cv_30d'] = np.where(features_df['rec_mean_30d'] > 0,\n",
    "                                         features_df['rec_std_30d'] / features_df['rec_mean_30d'], 0).astype(np.float32)\n",
    "    features_df['rec_zero_ratio_30d'] = (1.0 - (features_df.get('rec_count_30d', 0) / 30.0)).astype(np.float32)\n",
    "\n",
    "    # YoY same-window \n",
    "    rec_by_rm = {rm: g.sort_values('date_arrival').reset_index(drop=True)\n",
    "                 for rm, g in dr.groupby('rm_id')}\n",
    "    last_year_vals = np.zeros(len(features_df), dtype=float)\n",
    "    starts_ly = pd.to_datetime(features_df[\"forecast_start_date\"]) - pd.Timedelta(days=365)\n",
    "    ends_ly   = pd.to_datetime(features_df[\"forecast_end_date\"])   - pd.Timedelta(days=365)\n",
    "\n",
    "    for rm, rows in features_df.groupby('rm_id'):\n",
    "        r = rec_by_rm.get(rm)\n",
    "        if r is None or r.empty:\n",
    "            continue\n",
    "        dates = r['date_arrival'].values.astype('datetime64[D]')\n",
    "        w = r['net_weight'].values.astype(float)\n",
    "        c = np.cumsum(w)\n",
    "        L = np.searchsorted(dates, starts_ly.loc[rows.index].values.astype('datetime64[D]'), side='left')\n",
    "        R = np.searchsorted(dates, ends_ly.loc[rows.index].values.astype('datetime64[D]'),   side='right') - 1\n",
    "        s = np.zeros(len(rows), dtype=float)\n",
    "        for i, (l, rpos) in enumerate(zip(L, R)):\n",
    "            if rpos >= l and l < len(c):\n",
    "                s[i] = c[rpos] - (c[l-1] if l > 0 else 0.0)\n",
    "        last_year_vals[rows.index.values] = s\n",
    "\n",
    "    features_df[\"last_year_weight\"] = last_year_vals.astype(np.float32)\n",
    "\n",
    "    # Seasonal priors\n",
    "    # print(\"  Computing rm seasonal priors (global baseline)\")\n",
    "    dr['month'] = dr['date_arrival'].dt.month\n",
    "    dr['dow']   = dr['date_arrival'].dt.weekday\n",
    "\n",
    "    mprior = dr.groupby([\"rm_id\", \"month\"])[\"net_weight\"].mean()\n",
    "    mprior.name = \"rm_month_mean_prior\"\n",
    "    \n",
    "    dprior = dr.groupby([\"rm_id\", \"dow\"])[\"net_weight\"].mean()\n",
    "    dprior.name = \"rm_dow_mean_prior\"\n",
    "    \n",
    "    gprior = dr.groupby(\"month\")[\"net_weight\"].mean()\n",
    "    gprior.name = \"global_month_mean_prior\"\n",
    "\n",
    "    features_df = features_df.merge(mprior.reset_index(),\n",
    "                                   left_on=[\"rm_id\", \"end_month\"], right_on=[\"rm_id\", \"month\"], how=\"left\").drop(columns=[\"month\"])\n",
    "    features_df = features_df.merge(dprior.reset_index(),\n",
    "                                   left_on=[\"rm_id\", \"end_dayofweek\"], right_on=[\"rm_id\", \"dow\"], how=\"left\").drop(columns=[\"dow\"])\n",
    "    features_df = features_df.merge(gprior.reset_index().rename(columns={\"month\": \"end_month\"}),\n",
    "                                   on=\"end_month\", how=\"left\")\n",
    "\n",
    "    features_df[\"rm_month_mean_prior\"] = features_df[\"rm_month_mean_prior\"].fillna(0.0)\n",
    "    features_df[\"rm_dow_mean_prior\"]   = features_df[\"rm_dow_mean_prior\"].fillna(0.0)\n",
    "    features_df[\"global_month_mean_prior\"] = features_df[\"global_month_mean_prior\"].fillna(0.0)\n",
    "    features_df[\"rm_month_vs_global\"] = (features_df[\"rm_month_mean_prior\"] - features_df[\"global_month_mean_prior\"]).astype(np.float32)\n",
    "\n",
    "    # typical load size and expected truck count\n",
    "    load_stats = (dr.groupby([\"rm_id\", \"date_arrival\"])[\"net_weight\"].sum()\n",
    "                      .reset_index()\n",
    "                      .query(\"net_weight > 0\"))\n",
    "    \n",
    "    typical_load = load_stats.groupby(\"rm_id\")[\"net_weight\"].median()\n",
    "    typical_load.name = \"typical_load_rm\"\n",
    "    \n",
    "    features_df = features_df.merge(typical_load.reset_index(), on=\"rm_id\", how=\"left\")\n",
    "    global_median_load = float(load_stats[\"net_weight\"].median()) if len(load_stats) else 0.0\n",
    "    features_df[\"typical_load_rm\"] = features_df[\"typical_load_rm\"].fillna(global_median_load).astype(np.float32)\n",
    "    features_df[\"expected_truck_count\"] = (features_df[\"po_in_window\"] / (features_df[\"typical_load_rm\"] + 1e-6)).clip(lower=0).astype(np.float32)\n",
    "\n",
    "    # true lead-time and supplier reliability\n",
    "    if (receivals_full is not None) and (purchase_orders_full is not None):\n",
    "        # print(\"  Calculating: True lead-time and supplier reliability  \")\n",
    "        rf = receivals_full.copy()\n",
    "        pf = purchase_orders_full.copy()\n",
    "\n",
    "        rf[\"date_arrival\"]  = pd.to_datetime(rf[\"date_arrival\"], errors=\"coerce\")\n",
    "        pf[\"delivery_date\"] = pd.to_datetime(pf[\"delivery_date\"], errors=\"coerce\")\n",
    "\n",
    "        join_keys = []\n",
    "        if \"purchase_order_id\" in rf.columns and \"purchase_order_item_no\" in rf.columns:\n",
    "            join_keys = [\"purchase_order_id\", \"purchase_order_item_no\"]\n",
    "        elif \"purchase_order_id\" in rf.columns:\n",
    "            join_keys = [\"purchase_order_id\"]\n",
    "\n",
    "        if join_keys:\n",
    "            jt = rf.merge(\n",
    "                pf[join_keys + [c for c in [\"product_id\", \"delivery_date\", \"quantity\"] if c in pf.columns]],\n",
    "                on=join_keys, how=\"left\", suffixes=(\"\", \"_po\")\n",
    "            )\n",
    "            if \"product_id_po\" in jt.columns and \"product_id\" in jt.columns:\n",
    "                mask_pid = jt[\"product_id_po\"].isna() | (jt[\"product_id_po\"] == jt[\"product_id\"])\n",
    "                jt = jt[mask_pid]\n",
    "\n",
    "            jt = jt.dropna(subset=[\"date_arrival\", \"delivery_date\"])\n",
    "            jt[\"lead_time_days\"] = (jt[\"date_arrival\"] - jt[\"delivery_date\"]).dt.days\n",
    "            jt = jt[(jt[\"lead_time_days\"] >= -10) & (jt[\"lead_time_days\"] <= 180)]\n",
    "\n",
    "            lt_rm = jt.groupby(\"rm_id\")[\"lead_time_days\"].agg([\"mean\", \"std\", \"median\"]).reset_index()\n",
    "            lt_rm.columns = [\"rm_id\", \"lead_time_mean_rm\", \"lead_time_std_rm\", \"lead_time_median_rm\"]\n",
    "            features_df = features_df.merge(lt_rm, on=\"rm_id\", how=\"left\")\n",
    "\n",
    "            # supplier reliability \n",
    "            if \"supplier_id\" in rf.columns:\n",
    "                lt_sup = jt.groupby(\"supplier_id\")[\"lead_time_days\"].agg([\"mean\", \"std\", \"median\"]).reset_index()\n",
    "                lt_sup.columns = [\"supplier_id\", \"sup_lead_mean\", \"sup_lead_std\", \"sup_lead_median\"]\n",
    "\n",
    "                dom_sup = (rf.groupby([\"rm_id\", \"supplier_id\"])[\"net_weight\"].sum()\n",
    "                             .reset_index()\n",
    "                             .sort_values([\"rm_id\", \"net_weight\"], ascending=[True, False])\n",
    "                             .drop_duplicates([\"rm_id\"])\n",
    "                             .rename(columns={\"supplier_id\": \"dominant_supplier_id\"}))[[\"rm_id\", \"dominant_supplier_id\"]]\n",
    "\n",
    "                features_df = features_df.merge(dom_sup, on=\"rm_id\", how=\"left\")\n",
    "                features_df = features_df.merge(lt_sup, left_on=\"dominant_supplier_id\", right_on=\"supplier_id\", how=\"left\")\n",
    "                if \"supplier_id\" in features_df.columns:\n",
    "                    features_df.drop(columns=[\"supplier_id\"], inplace=True)\n",
    "\n",
    "                features_df[\"supplier_reliability_score\"] = (\n",
    "                    (-features_df[\"sup_lead_mean\"].fillna(0.0) / 30.0) +\n",
    "                    (-features_df[\"sup_lead_std\"].fillna(0.0)  / 15.0)\n",
    "                ).astype(np.float32)\n",
    "\n",
    "            for c in [\"lead_time_mean_rm\", \"lead_time_std_rm\", \"lead_time_median_rm\",\n",
    "                      \"sup_lead_mean\", \"sup_lead_std\", \"sup_lead_median\", \"supplier_reliability_score\"]:\n",
    "                if c in features_df.columns:\n",
    "                    features_df[c] = features_df[c].fillna(0.0).astype(np.float32)\n",
    "        else:\n",
    "            for c in [\"lead_time_mean_rm\", \"lead_time_std_rm\", \"lead_time_median_rm\",\n",
    "                      \"sup_lead_mean\", \"sup_lead_std\", \"sup_lead_median\", \"supplier_reliability_score\"]:\n",
    "                features_df[c] = 0.0 \n",
    "\n",
    "    #   Croston forecasting as features  \n",
    "    win_min = pd.to_datetime(features_df['forecast_start_date']).min()\n",
    "    win_max = pd.to_datetime(features_df['forecast_end_date']).max()\n",
    "    \n",
    "    if pd.isna(win_min) or pd.isna(win_max):\n",
    "        features_df['baseline_sum_in_window'] = 0.0\n",
    "    else:\n",
    "        baseline_df = build_intermittent_baseline(daily_receivals, win_min, win_max, method=\"croston\", alpha=0.1)\n",
    "        features_df['baseline_sum_in_window'] = sum_baseline_over_windows(features_df, baseline_df).astype(np.float32)\n",
    "\n",
    "    core_feature_cols = [\n",
    "        'rm_id',\n",
    "        'window_length',\n",
    "\n",
    "        # calendar\n",
    "        'end_dayofweek', 'end_days_to_month_end',\n",
    "        'business_days_in_window', 'weekends_in_window',\n",
    "        'month_sin', 'month_cos',\n",
    "        'baseline_sum_in_window',\n",
    "\n",
    "        # PO window\n",
    "        'po_in_window', 'daily_avg_po_in_window',\n",
    "\n",
    "        # historical sums\n",
    "        'hist_rec_7d', 'hist_rec_30d', 'hist_rec_90d',\n",
    "        'hist_po_7d',  'hist_po_30d',  'hist_po_90d',\n",
    "\n",
    "        # rolling stats and trend\n",
    "        'rec_mean_30d', 'rec_std_30d', 'rec_trend_30d',\n",
    "        'rec_mean_90d', 'rec_std_90d', 'rec_trend_90d',\n",
    "\n",
    "        # counts and sparsity\n",
    "        'rec_count_30d', 'rec_count_90d', 'rec_zero_ratio_30d',\n",
    "\n",
    "        # ratios\n",
    "        'po_to_rec_ratio_30d', 'rec_cv_30d',\n",
    "\n",
    "        # priors & YoY\n",
    "        'last_year_weight', 'rm_month_mean_prior', 'rm_dow_mean_prior', 'rm_month_vs_global',\n",
    "\n",
    "        # logistics\n",
    "        'typical_load_rm', 'expected_truck_count',\n",
    "\n",
    "        # ead-time and supplier reliability\n",
    "        'lead_time_mean_rm', 'lead_time_std_rm', 'lead_time_median_rm',\n",
    "        'supplier_reliability_score'\n",
    "    ]\n",
    "\n",
    "    final_feature_cols = core_feature_cols + ['index', 'forecast_start_date', 'forecast_end_date']\n",
    "    \n",
    "    # if skip then fill with zeross\n",
    "    for col in core_feature_cols:\n",
    "        if col not in features_df.columns:\n",
    "            features_df[col] = 0.0\n",
    "            \n",
    "    return features_df[final_feature_cols].set_index('index')\n",
    "\n",
    "    \n",
    "def training_data(daily_receivals, daily_po, year_for_training, receivals_full=None, purchase_orders_full=None):\n",
    "\n",
    "    windows_for_train = []\n",
    "    all_rm_ids = daily_receivals['rm_id'].unique()\n",
    "    for rm_id in all_rm_ids:\n",
    "        for year in year_for_training:\n",
    "            start_date = date(year, 1, 1)\n",
    "            end_max = date(year, 5, 31)\n",
    "            max_horizon = (end_max - start_date).days  \n",
    "            for h in range(0, max_horizon + 1): \n",
    "                end_date = start_date + timedelta(days=h)\n",
    "                windows_for_train.append({'rm_id': rm_id, 'forecast_start_date': start_date, 'forecast_end_date': end_date})\n",
    "    raw_train = pd.DataFrame(windows_for_train)\n",
    "\n",
    "    X_train = create_features(raw_train, daily_receivals, daily_po,\n",
    "                              receivals_full=receivals_full, purchase_orders_full=purchase_orders_full)\n",
    " \n",
    "    # target \n",
    "    y_train = pd.Series(0.0, index=X_train.index, name='target', dtype=float)\n",
    "    daily_receivals_copy = daily_receivals.copy()\n",
    "    if not np.issubdtype(daily_receivals_copy['date_arrival'].dtype, np.datetime64):\n",
    "        daily_receivals_copy['date_arrival'] = pd.to_datetime(daily_receivals_copy['date_arrival'], errors='coerce')\n",
    "    receivals_lookup_by_rm = {rm: grp.sort_values('date_arrival').reset_index(drop=True)\n",
    "                              for rm, grp in daily_receivals_copy.groupby('rm_id')}\n",
    "\n",
    "    for rm_id, rows in X_train.reset_index().groupby('rm_id'):\n",
    "        rows_index = rows['index'].values\n",
    "        receivals_group = receivals_lookup_by_rm.get(rm_id)\n",
    "        if receivals_group is None or receivals_group.empty: continue\n",
    "        \n",
    "        rec_dates = receivals_group['date_arrival'].values.astype('datetime64[D]')\n",
    "        rec_weights = receivals_group['net_weight'].values.astype(float)\n",
    "        rec_cumulative = np.cumsum(rec_weights)\n",
    "        starts = pd.to_datetime(rows['forecast_start_date']).values.astype('datetime64[D]')\n",
    "        ends = pd.to_datetime(rows['forecast_end_date']).values.astype('datetime64[D]')\n",
    "        left_pos = np.searchsorted(rec_dates, starts, side='left')\n",
    "        right_pos = np.searchsorted(rec_dates, ends, side='right') - 1\n",
    "\n",
    "        sums = np.zeros(len(rows_index), dtype=float)\n",
    "        for i, (l, r) in enumerate(zip(left_pos, right_pos)):\n",
    "            if r >= l and r >= 0 and l < len(rec_cumulative):\n",
    "                left_cum = rec_cumulative[l-1] if l > 0 else 0.0\n",
    "                sums[i] = rec_cumulative[r] - left_cum\n",
    "            else:\n",
    "                sums[i] = 0.0 \n",
    "        y_train.loc[rows_index] = sums\n",
    "\n",
    "    X_train_export = X_train.drop(columns=['forecast_start_date', 'forecast_end_date'], errors='ignore')\n",
    "    return X_train_export, y_train\n",
    "\n",
    "def run_data_preparation():\n",
    "\n",
    "    loaded_data = load_data()\n",
    "    receivals, purchase_orders, materials, prediction_mapping = loaded_data\n",
    "    daily_receivals, daily_po = aggregates_3df_daily(receivals, purchase_orders, materials)\n",
    "\n",
    "    min_year = receivals['date_arrival'].min().year\n",
    "    max_year = 2024\n",
    "    all_years = list(range(min_year, max_year + 1))\n",
    "    covid_years = {2020, 2021, 2022} #covid years\n",
    "    training_years = [year for year in all_years if year not in covid_years]\n",
    "\n",
    "    \n",
    "    X_train, y_train = training_data(\n",
    "        daily_receivals, daily_po, year_for_training=training_years,\n",
    "        receivals_full=receivals, purchase_orders_full=purchase_orders\n",
    "    )\n",
    "\n",
    "    # print(\"Exporting training dataset  \")\n",
    "    training_data_to_export = X_train.copy()\n",
    "    training_data_to_export['target'] = y_train\n",
    "    return training_data_to_export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(daily_receivals, daily_po, year_for_training, receivals_full=None, purchase_orders_full=None):\n",
    "\n",
    "    windows_for_train = []\n",
    "    all_rm_ids = daily_receivals['rm_id'].unique()\n",
    "\n",
    "    for rm_id in all_rm_ids:\n",
    "        for year in year_for_training:\n",
    "            start_date = date(year, 1, 1)\n",
    "            end_max = date(year, 5, 31)\n",
    "            max_horizon = (end_max - start_date).days  \n",
    "            \n",
    "            for h in range(0, max_horizon + 1): \n",
    "                end_date = start_date + timedelta(days=h)\n",
    "                windows_for_train.append({\n",
    "                    'rm_id': rm_id,\n",
    "                    'forecast_start_date': start_date,\n",
    "                    'forecast_end_date': end_date\n",
    "                })\n",
    "\n",
    "    raw_train = pd.DataFrame(windows_for_train)\n",
    "\n",
    "    # Build features\n",
    "    X_train = create_features(raw_train, daily_receivals, daily_po, receivals_full=receivals_full, purchase_orders_full=purchase_orders_full)\n",
    "\n",
    "    y_train = pd.Series(0.0, index=X_train.index, name='target', dtype=float)\n",
    "    daily_receivals_copy = daily_receivals.copy()\n",
    "     \n",
    "    if not np.issubdtype(daily_receivals_copy['date_arrival'].dtype, np.datetime64): # datetime check\n",
    "        daily_receivals_copy['date_arrival'] = pd.to_datetime(daily_receivals_copy['date_arrival'], errors='coerce')\n",
    "        \n",
    "    receivals_lookup_by_rm = {rm: grp.sort_values('date_arrival').reset_index(drop=True)\n",
    "                              for rm, grp in daily_receivals_copy.groupby('rm_id')}\n",
    "\n",
    "    for rm_id, rows in X_train.reset_index().groupby('rm_id'):\n",
    "        rows_index = rows['index'].values\n",
    "        receivals_group = receivals_lookup_by_rm.get(rm_id)\n",
    "        \n",
    "        if receivals_group is None or receivals_group.empty:\n",
    "            continue\n",
    "            \n",
    "        rec_dates = receivals_group['date_arrival'].values.astype('datetime64[D]')\n",
    "        rec_weights = receivals_group['net_weight'].values.astype(float)\n",
    "        rec_cumulative = np.cumsum(rec_weights)\n",
    "        \n",
    "        starts = pd.to_datetime(rows['forecast_start_date']).values.astype('datetime64[D]')\n",
    "        ends = pd.to_datetime(rows['forecast_end_date']).values.astype('datetime64[D]')\n",
    "        left_pos = np.searchsorted(rec_dates, starts, side='left')\n",
    "        right_pos = np.searchsorted(rec_dates, ends, side='right') - 1\n",
    "\n",
    "        sums = np.zeros(len(rows_index), dtype=float)\n",
    "        # calculate sums\n",
    "        for i, (l, r) in enumerate(zip(left_pos, right_pos)):\n",
    "            if r >= l and r >= 0 and l < len(rec_cumulative):\n",
    "                left_cum = rec_cumulative[l-1] if l > 0 else 0.0\n",
    "                sums[i] = rec_cumulative[r] - left_cum\n",
    "            else:\n",
    "                sums[i] = 0.0\n",
    "        y_train.loc[rows_index] = sums\n",
    "\n",
    "    # drop of helper columns\n",
    "    X_train_export = X_train.drop(columns=['forecast_start_date', 'forecast_end_date'], errors='ignore')\n",
    "    # print(\"Training data and targets generated successfully.\")\n",
    "    return X_train_export, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_rm_id_as_codes(X_train_full: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame):\n",
    "    # rm_id categorical then map to stable int codes for XGBoost.\n",
    "\n",
    "    if 'rm_id' not in X_train_full.columns:\n",
    "        return X_train_full, X_val, X_test, []\n",
    "\n",
    "    # print(\"Encoding rm_id as categorical codes  \")\n",
    "    X_train_full = X_train_full.copy()\n",
    "    X_train_full['rm_id'] = X_train_full['rm_id'].astype('category')\n",
    "    rm_categories = X_train_full['rm_id'].cat.categories.tolist()\n",
    "\n",
    "    def _align_categories(df):\n",
    "        df = df.copy()\n",
    "        df['rm_id'] = pd.Categorical(df['rm_id'], categories=rm_categories)\n",
    "        df['rm_id'] = df['rm_id'].cat.codes.astype('int32') # Use codes for XGB\n",
    "        return df\n",
    "\n",
    "    X_val  = _align_categories(X_val)\n",
    "    X_test = _align_categories(X_test)\n",
    "    X_train_full['rm_id'] = X_train_full['rm_id'].cat.codes.astype('int32')\n",
    "\n",
    "    return X_train_full, X_val, X_test, rm_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost Pipeline\n",
      "Loaded training data: 552769 rows, 38 features.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting XGBoost Pipeline\")\n",
    " \n",
    "training_df = run_data_preparation()\n",
    "\n",
    "# Separate target and weights (if they exist)\n",
    "sample_weight_series = None\n",
    "if 'recency_weight' in training_df.columns:\n",
    "    sample_weight_series = training_df['recency_weight']\n",
    "\n",
    "# Drop meta cols\n",
    "drop_meta = [c for c in ['forecast_start_date', 'forecast_end_date', 'recency_weight'] \n",
    "             if c in training_df.columns]\n",
    "X_train = training_df.drop(columns=['target'] + drop_meta, errors='ignore')\n",
    "y_train = training_df['target']\n",
    "\n",
    "print(f\"Loaded training data: {X_train.shape[0]} rows, {X_train.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data to build test set  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build test set  \n",
    "print(\"Loading raw data to build test set  \")\n",
    "data_tuple = load_data()\n",
    "if data_tuple[0] is None:\n",
    "    raise FileNotFoundError(\"Raw data loading failed for test set creation.\")\n",
    "\n",
    "df_rec, purchase_orders, materials, prediction_mapping = data_tuple\n",
    "daily_receivals, daily_po = aggregates_3df_daily(df_rec, purchase_orders, materials)\n",
    "\n",
    "test_df_raw = prediction_mapping.set_index('ID')\n",
    "\n",
    "X_test = create_features(\n",
    "    test_df_raw, daily_receivals, daily_po,\n",
    "    receivals_full=df_rec, purchase_orders_full=purchase_orders\n",
    ")\n",
    "\n",
    "# align test columns to train columns\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating chronological validation set (last 150 rows per rm_id)  \n"
     ]
    }
   ],
   "source": [
    "print(\"Creating chronological validation set (last 150 rows per rm_id)  \")\n",
    "val_idx = X_train.groupby('rm_id').tail(150).index\n",
    "X_val = X_train.loc[val_idx].copy()\n",
    "y_val = y_train.loc[val_idx].copy()\n",
    "\n",
    "train_idx = X_train.index.difference(val_idx)\n",
    "X_train_full = X_train.loc[train_idx].copy()\n",
    "y_train_full = y_train.loc[train_idx].copy()\n",
    "\n",
    "# print(f\"Training rows: {len(X_train_full)}, Validation rows: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode rm_id as integer codes\n",
    "X_train_full, X_val, X_test, rm_categories = _encode_rm_id_as_codes(X_train_full, X_val, X_test)\n",
    "\n",
    "# sample weights\n",
    "if sample_weight_series is None:\n",
    "    sample_weight_series = pd.Series(1.0, index=X_train.index)\n",
    "else:\n",
    "    sample_weight_series = sample_weight_series.reindex(X_train.index).fillna(1.0)\n",
    "\n",
    "sample_weight_val   = sample_weight_series.loc[val_idx].values\n",
    "sample_weight_train = sample_weight_series.loc[train_idx].values\n",
    "\n",
    "# QuantileDMatrix datasets\n",
    "dtrain = xgb.QuantileDMatrix(X_train_full, y_train_full, weight=sample_weight_train)\n",
    "dvalid = xgb.QuantileDMatrix(X_val, y_val, weight=sample_weight_val, ref=dtrain)\n",
    "dtest  = xgb.QuantileDMatrix(X_test, ref=dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:20:21,595] A new study created in memory with name: no-name-66ff24f3-4444-450e-b738-d2c7777aafd0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna hyperparameter tuning (20 trials)  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:22:32,349] Trial 0 finished with value: 14716.869282288972 and parameters: {'eta': 0.008468008575248327, 'max_depth': 12, 'min_child_weight': 6.110836758879259, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182, 'reg_alpha': 2.5348407664333426e-07, 'reg_lambda': 3.3323645788192616e-08, 'gamma': 8.661761457749352}. Best is trial 0 with value: 14716.869282288972.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:23:43,397] Trial 1 finished with value: 13999.656777253667 and parameters: {'eta': 0.030834348179355788, 'max_depth': 10, 'min_child_weight': 0.011977006629986053, 'subsample': 0.9849549260809971, 'colsample_bytree': 0.9162213204002109, 'reg_alpha': 8.148018307012941e-07, 'reg_lambda': 4.329370014459266e-07, 'gamma': 1.8340450985343382}. Best is trial 1 with value: 13999.656777253667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:25:46,681] Trial 2 finished with value: 13393.617517774343 and parameters: {'eta': 0.005670807781371429, 'max_depth': 8, 'min_child_weight': 0.44061621941666496, 'subsample': 0.645614570099021, 'colsample_bytree': 0.8059264473611898, 'reg_alpha': 1.8007140198129195e-07, 'reg_lambda': 4.258943089524393e-06, 'gamma': 3.663618432936917}. Best is trial 2 with value: 13393.617517774343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:26:18,969] Trial 3 finished with value: 13159.766692753547 and parameters: {'eta': 0.013481575603601416, 'max_depth': 10, 'min_child_weight': 0.05754324524386239, 'subsample': 0.7571172192068059, 'colsample_bytree': 0.7962072844310213, 'reg_alpha': 2.6185068507773707e-08, 'reg_lambda': 0.0029369981104377003, 'gamma': 1.7052412368729153}. Best is trial 3 with value: 13159.766692753547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:29:20,709] Trial 4 finished with value: 15150.38855153888 and parameters: {'eta': 0.0014492412389916862, 'max_depth': 12, 'min_child_weight': 47.35537788436196, 'subsample': 0.9041986740582306, 'colsample_bytree': 0.6523068845866853, 'reg_alpha': 7.569183361880229e-08, 'reg_lambda': 0.014391207615728067, 'gamma': 4.4015249373960135}. Best is trial 3 with value: 13159.766692753547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:31:37,082] Trial 5 finished with value: 13337.759090754727 and parameters: {'eta': 0.002005873336344495, 'max_depth': 7, 'min_child_weight': 0.013517267252658744, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085, 'reg_alpha': 0.009176996354542699, 'reg_lambda': 6.388511557344611e-06, 'gamma': 5.200680211778108}. Best is trial 3 with value: 13159.766692753547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:32:08,696] Trial 6 finished with value: 12867.018278318572 and parameters: {'eta': 0.02260828676373495, 'max_depth': 4, 'min_child_weight': 49.024547442450555, 'subsample': 0.8875664116805573, 'colsample_bytree': 0.9697494707820946, 'reg_alpha': 1.1309571585271483, 'reg_lambda': 0.002404915432737351, 'gamma': 9.218742350231167}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:34:14,466] Trial 7 finished with value: 13775.02908164294 and parameters: {'eta': 0.0016565580440884786, 'max_depth': 4, 'min_child_weight': 0.014864256854557617, 'subsample': 0.6626651653816322, 'colsample_bytree': 0.6943386448447411, 'reg_alpha': 2.7678419414850017e-06, 'reg_lambda': 0.28749982347407854, 'gamma': 3.567533266935893}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:35:10,854] Trial 8 finished with value: 14454.062156101569 and parameters: {'eta': 0.0049648810171066555, 'max_depth': 8, 'min_child_weight': 0.0343861032550121, 'subsample': 0.9010984903770198, 'colsample_bytree': 0.5372753218398854, 'reg_alpha': 7.620481786158549, 'reg_lambda': 0.08916674715636537, 'gamma': 1.987156815341724}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:38:50,044] Trial 9 finished with value: 15575.333945722732 and parameters: {'eta': 0.0010319982330247674, 'max_depth': 11, 'min_child_weight': 4.902597806996491, 'subsample': 0.8645035840204937, 'colsample_bytree': 0.8856351733429728, 'reg_alpha': 4.638759594322625e-08, 'reg_lambda': 1.683416412018213e-05, 'gamma': 1.1586905952512971}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:39:01,480] Trial 10 finished with value: 13182.96627914518 and parameters: {'eta': 0.21813286105635582, 'max_depth': 3, 'min_child_weight': 49.00413584586065, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9919371080730973, 'reg_alpha': 1.475649304728371, 'reg_lambda': 4.3444691085504115, 'gamma': 9.767678566447309}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:39:17,488] Trial 11 finished with value: 13428.145726126497 and parameters: {'eta': 0.036635400935561155, 'max_depth': 5, 'min_child_weight': 0.22630390030625855, 'subsample': 0.7603013498857213, 'colsample_bytree': 0.797206172727708, 'reg_alpha': 0.00025578608535472305, 'reg_lambda': 0.0007126913292289021, 'gamma': 7.158552206765736}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:39:32,337] Trial 12 finished with value: 13509.83979672377 and parameters: {'eta': 0.07986531313992158, 'max_depth': 6, 'min_child_weight': 0.10380447470641005, 'subsample': 0.6823071444448356, 'colsample_bytree': 0.9685764864260473, 'reg_alpha': 0.016063101739857453, 'reg_lambda': 0.0006469778338748704, 'gamma': 6.526812002249811}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:39:54,396] Trial 13 finished with value: 13746.45061784811 and parameters: {'eta': 0.021207274009445345, 'max_depth': 9, 'min_child_weight': 2.055865004248282, 'subsample': 0.8223972680035054, 'colsample_bytree': 0.8556009490874331, 'reg_alpha': 0.00011900731732140921, 'reg_lambda': 0.003625907335716626, 'gamma': 0.43477855995199555}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:40:25,407] Trial 14 finished with value: 13766.764025565106 and parameters: {'eta': 0.0107568974772929, 'max_depth': 10, 'min_child_weight': 0.0729473675676556, 'subsample': 0.7307024197434439, 'colsample_bytree': 0.7647130385476281, 'reg_alpha': 0.07958086472539412, 'reg_lambda': 6.71983944238818e-05, 'gamma': 7.380983421262327}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:40:38,419] Trial 15 finished with value: 14073.576712810851 and parameters: {'eta': 0.08022969997911299, 'max_depth': 3, 'min_child_weight': 14.79235495296152, 'subsample': 0.5827781406926889, 'colsample_bytree': 0.7261631564388042, 'reg_alpha': 1.5900607832650244e-05, 'reg_lambda': 0.017016149251141185, 'gamma': 5.726110230006725}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:41:06,564] Trial 16 finished with value: 13447.977059219598 and parameters: {'eta': 0.013134924547445997, 'max_depth': 6, 'min_child_weight': 1.080195015022487, 'subsample': 0.8237388893476809, 'colsample_bytree': 0.9357564677880447, 'reg_alpha': 0.002583197184230675, 'reg_lambda': 1.9866914870533452, 'gamma': 2.7504063681238393}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:41:21,698] Trial 17 finished with value: 13573.121488069139 and parameters: {'eta': 0.062144905190434516, 'max_depth': 9, 'min_child_weight': 0.2866703179218495, 'subsample': 0.7486682444821691, 'colsample_bytree': 0.8582453381144602, 'reg_alpha': 0.2547577913404216, 'reg_lambda': 0.00013171270825740902, 'gamma': 9.81150897216655}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:42:24,488] Trial 18 finished with value: 13080.174776596517 and parameters: {'eta': 0.0035545030121162256, 'max_depth': 5, 'min_child_weight': 17.419208713304794, 'subsample': 0.90928355079006, 'colsample_bytree': 0.8214237152952776, 'reg_alpha': 2.1220102605209543e-05, 'reg_lambda': 0.004238580091270327, 'gamma': 8.257569030919115}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-09 13:43:30,410] Trial 19 finished with value: 13599.432224058028 and parameters: {'eta': 0.003423379766714161, 'max_depth': 5, 'min_child_weight': 18.50330376124951, 'subsample': 0.9329003770062791, 'colsample_bytree': 0.9512742826174833, 'reg_alpha': 1.3865505452822999e-05, 'reg_lambda': 0.23378225464218721, 'gamma': 8.15729837309543}. Best is trial 6 with value: 12867.018278318572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Optuna found: best params: {'eta': 0.02260828676373495, 'max_depth': 4, 'min_child_weight': 49.024547442450555, 'subsample': 0.8875664116805573, 'colsample_bytree': 0.9697494707820946, 'reg_alpha': 1.1309571585271483, 'reg_lambda': 0.002404915432737351, 'gamma': 9.218742350231167}, best score: 12867.018278318572, in 20 trials.\n"
     ]
    }
   ],
   "source": [
    "### UNCOMMENT THE FOLLOWING TO ENABLE GPU USAGE ###\n",
    "# it requires device= xgb_device in params!!\n",
    "\n",
    "# use_gpu = os.system(\"nvidia-smi > /dev/null 2>&1\") == 0\n",
    "# if use_gpu:\n",
    "#     print(\"\\nGPU Rilevata! XGBoost user 'cuda'.\")\n",
    "#     xgb_device = \"cuda\"\n",
    "# else:\n",
    "#     print(\"\\nNessuna GPU rilevata (o driver non trovato). XGBoost user la CPU.\")\n",
    "#     xgb_device = \"cpu\"\n",
    "\n",
    "\n",
    "base_params = {\n",
    "    \"objective\": \"reg:quantileerror\",\n",
    "    \"quantile_alpha\": 0.2,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eval_metric\": \"quantile\",\n",
    "    \"seed\": SEED,\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "\n",
    "best_params = {}\n",
    "best_iter = None\n",
    "best_score_for_log = None\n",
    "n_trials_for_log = 0\n",
    "\n",
    "\n",
    "print(f\"Starting Optuna hyperparameter tuning ({HYPEROPT_N_TRIALS} trials)  \")\n",
    "\n",
    "def objective(trial: \"optuna.trial.Trial\"):\n",
    "    params = base_params.copy()\n",
    "    params.update({\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-3, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 64.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 10.0),\n",
    "    })\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=20000,\n",
    "        evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    trial.set_user_attr(\"best_iteration\", booster.best_iteration)\n",
    "    trial.set_user_attr(\"best_score\", booster.best_score)\n",
    "    print(booster.attributes().get('scikit_learn', {}).get('device'))\n",
    "    return float(booster.best_score)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=HYPEROPT_N_TRIALS, timeout=None)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_iter = int(study.best_trial.user_attrs.get(\"best_iteration\", 1000))\n",
    "best_score_for_log = study.best_value\n",
    "n_trials_for_log = len(study.trials)\n",
    "\n",
    "print(f\"Optuna found: best params: {best_params}, best score: {best_score_for_log}, in {n_trials_for_log} trials.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining final model on all training data (train + validation)\n",
      "Training final model for 375 iterations\n",
      "Final model training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Retraining final model on all training data (train + validation)\")\n",
    "X_combined = pd.concat([X_train_full, X_val], axis=0)\n",
    "y_combined = pd.concat([y_train_full, y_val], axis=0)\n",
    "sw_combined = pd.concat(\n",
    "    [pd.Series(sample_weight_train, index=X_train_full.index),\n",
    "     pd.Series(sample_weight_val, index=X_val.index)],\n",
    "    axis=0\n",
    ").reindex(X_combined.index).fillna(1.0).values\n",
    "\n",
    "dcomb = xgb.QuantileDMatrix(X_combined, y_combined, weight=sw_combined)\n",
    "\n",
    "final_params = base_params.copy()\n",
    "final_params.update(best_params)\n",
    "\n",
    "final_num_boost_round = (best_iter + 1 if best_iter is not None else 1000)\n",
    "print(f\"Training final model for {final_num_boost_round} iterations\")\n",
    "\n",
    "booster_final = xgb.train(\n",
    "    params=final_params,\n",
    "    dtrain=dcomb,\n",
    "    num_boost_round=final_num_boost_round,\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "print(\"Final model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Saving\n",
    "\n",
    "Uncomment to plot feature importance, save model metadata and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importance(booster, feature_names, top_n=30):\n",
    "#     print(\"\\n  Feature Importance (Top 30 by 'Gain')  \")\n",
    "#     importance = booster.get_score(importance_type='gain')\n",
    "#     feature_map = {f'f{i}': name for i, name in enumerate(feature_names)}\n",
    "#     importance = {feature_map.get(k, k): v for k, v in importance.items()}\n",
    "\n",
    "#     importance_df = pd.DataFrame({\n",
    "#         'feature': importance.keys(),\n",
    "#         'importance': importance.values()\n",
    "#     })\n",
    "    \n",
    "#     importance_df = importance_df.sort_values(by='importance', ascending=False).head(top_n)\n",
    "    \n",
    "#     plt.figure(figsize=(10, top_n / 2.5))\n",
    "#     sns.barplot(x='importance', y='feature', data=importance_df, palette='viridis')\n",
    "#     plt.title(f'Top {top_n} Feature Importances (Type: Gain)')\n",
    "#     plt.xlabel('Importance (Gain)')\n",
    "#     plt.ylabel('Feature')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# plot_feature_importance(booster_final, X_train_full.columns, top_n=30)\n",
    "\n",
    "\n",
    "# print(\"Logging metrics to score_log.csv  \")\n",
    "# log_file_path = '../score_log.csv'\n",
    "# log_entry = {\n",
    "#     \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "#     \"best_validation_score\": best_score_for_log,\n",
    "#     \"best_iteration\": best_iter,\n",
    "#     \"n_trials\": n_trials_for_log,\n",
    "#     \"features_used\": \"|\".join(X_train.columns.tolist()) \n",
    "# }\n",
    "# log_df = pd.DataFrame([log_entry])\n",
    "\n",
    "\n",
    "# if not os.path.exists(log_file_path):\n",
    "#     log_df.to_csv(log_file_path, index=False)\n",
    "# else:\n",
    "#     log_df.to_csv(log_file_path, mode='a', header=False, index=False)\n",
    "# print(f\"Metrics successfully logged to {log_file_path}\")\n",
    "\n",
    "# model_file = '../xgb_model.json'\n",
    "# meta_file  = '../xgb_model_meta.json'\n",
    "\n",
    "# booster_final.save_model(str(model_file))\n",
    "\n",
    "# meta = {\n",
    "#     'feature_columns': X_train.columns.tolist(),\n",
    "#     'categorical_rm_id_categories': rm_categories,\n",
    "#     'quantile_alpha': 0.2,\n",
    "#     'library': 'xgboost',\n",
    "#     'best_iteration': int(best_iter) if best_iter is not None else None,\n",
    "#     'best_params': best_params,\n",
    "#     'best_validation_score': best_score_for_log\n",
    "# }\n",
    "# with open(meta_file, 'w', encoding='utf-8') as fh:\n",
    "#     json.dump(meta, fh, indent=2)\n",
    "\n",
    "# print(f\"Final model saved to {model_file}\")\n",
    "# print(f\"Metadata saved to {meta_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test set\n",
      "Submission file saved to ../final_submission_xgboost.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating predictions on test set\")\n",
    "\n",
    "preds = booster_final.predict(dtest)\n",
    "preds = np.asarray(preds, dtype=float)\n",
    "preds[preds < 0] = 0.0  \n",
    "\n",
    "submission = pd.DataFrame({'ID': X_test.index, 'predicted_weight': preds})\n",
    "mapping_small = prediction_mapping[['ID', 'rm_id', 'forecast_end_date']].copy()\n",
    "mapping_small['forecast_end_date'] = pd.to_datetime(mapping_small['forecast_end_date'])\n",
    "\n",
    "merged_sub = submission.merge(mapping_small, on='ID', how='left').sort_values(['rm_id', 'forecast_end_date'])\n",
    "\n",
    "merged_sub['predicted_weight'] = merged_sub.groupby('rm_id')['predicted_weight'].cummax()\n",
    "submission = merged_sub.sort_values('ID')[['ID', 'predicted_weight']].reset_index(drop=True)\n",
    "\n",
    "# Save submission\n",
    "submission_filepath = '../final_submission_xgboost.csv'\n",
    "submission.to_csv(submission_filepath, index=False)\n",
    "print(f\"Submission file saved to {submission_filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
